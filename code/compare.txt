/usr/lib/python2.7/dist-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead
  DeprecationWarning)
/usr/lib/python2.7/dist-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead
  DeprecationWarning)
/usr/lib/python2.7/dist-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead
  DeprecationWarning)
/usr/lib/python2.7/dist-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead
  DeprecationWarning)
/usr/lib/python2.7/dist-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead
  DeprecationWarning)
/usr/lib/python2.7/dist-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead
  DeprecationWarning)
/usr/lib/python2.7/dist-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead
  DeprecationWarning)
/usr/lib/python2.7/dist-packages/scipy/lib/_util.py:35: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead
  DeprecationWarning)
Using gpu device 0: GRID K520
loading data
Training for Protists

Train set size: 1620 examples.
Test set size: 179 inputs.

Train set size: 1620 examples.
Test set size: 179 inputs.

Train set size: 1620 examples.
Test set size: 179 inputs.

Train set size: 1620 examples.
Test set size: 179 inputs.

Train set size: 1620 examples.
Test set size: 179 inputs.

Train set size: 1620 examples.
Test set size: 179 inputs.

Train set size: 1620 examples.
Test set size: 179 inputs.

Train set size: 1620 examples.
Test set size: 179 inputs.
learning started
learning started
learning started
learning started
learning started
learning started
learning started

The optimal algorithm for GaussianNB is: GaussianNB()


Fitted GaussianNB with training scores 0.249382716049

The optimal algorithm for MultinomialNB is: MultinomialNB(alpha=0.001, class_prior=None, fit_prior=True)


Fitted MultinomialNB with training scores 0.538271604938

The optimal algorithm for kNN is: KNeighborsClassifier(algorithm=auto, leaf_size=30, metric=minkowski,
           n_neighbors=5, p=2, weights=distance)


Fitted kNN with training scores 0.675925925926

The optimal algorithm for SGD is: SGDClassifier(alpha=0.001, class_weight=None, epsilon=0.1, eta0=0.0,
       fit_intercept=True, l1_ratio=0.15, learning_rate=optimal,
       loss=modified_huber, n_iter=5, n_jobs=4, penalty=l1, power_t=0.5,
       random_state=None, rho=None, shuffle=False, verbose=0,
       warm_start=False)


Fitted SGD with training scores 0.506790123457

The optimal algorithm for AdaBoost is: AdaBoostClassifier(algorithm=SAMME,
          base_estimator=DecisionTreeClassifier(compute_importances=None, criterion=gini,
            max_depth=None, max_features=None, min_density=None,
            min_samples_leaf=1, min_samples_split=2, random_state=None,
            splitter=best),
          base_estimator__compute_importances=None,
          base_estimator__criterion=gini, base_estimator__max_depth=None,
          base_estimator__max_features=None,
          base_estimator__min_density=None,
          base_estimator__min_samples_leaf=1,
          base_estimator__min_samples_split=2,
          base_estimator__random_state=None, base_estimator__splitter=best,
          learning_rate=1.0, n_estimators=25, random_state=None)


Fitted AdaBoost with training scores 0.438888888889

The optimal algorithm for Decision Tree is: DecisionTreeClassifier(compute_importances=None, criterion=entropy,
            max_depth=16, max_features=sqrt, min_density=None,
            min_samples_leaf=4, min_samples_split=16, random_state=None,
            splitter=best)


Fitted Decision Tree with training scores 0.533333333333

The optimal algorithm for Logistic Classifier is: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, penalty=l2, random_state=None, tol=0.0001)


Fitted Logistic Classifier with training scores 0.596296296296
